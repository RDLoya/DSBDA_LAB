{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27ba99ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f80c82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc52843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language. It involves the application of computational techniques to analyze and understand human language. NLP can be used for various tasks such as machine translation, sentiment analysis, information extraction, and question answering.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ade3e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing NLP is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language It involves the application of computational techniques to analyze and understand human language NLP can be used for various tasks such as machine translation sentiment analysis information extraction and question answering'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Remove punctuation characters\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text\n",
    "\n",
    "document = remove_punctuation(document)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1382e6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', 'It', 'involves', 'the', 'application', 'of', 'computational', 'techniques', 'to', 'analyze', 'and', 'understand', 'human', 'language', 'NLP', 'can', 'be', 'used', 'for', 'various', 'tasks', 'such', 'as', 'machine', 'translation', 'sentiment', 'analysis', 'information', 'extraction', 'and', 'question', 'answering']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'document_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21364\\4172771405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'document_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(document))\n",
    "print(document_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf789fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(document)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "for word, pos_tag in pos_tags:\n",
    "    print(f\"Word: {word}\\tPOS Tag: {pos_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  >>> import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06666c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d929b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_document = \" \".join(lemmatized_tokens)\n",
    "preprocessed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35f28249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis: 0.11785113019775793\n",
      "analyze: 0.11785113019775793\n",
      "and: 0.3535533905932738\n",
      "answering: 0.11785113019775793\n",
      "application: 0.11785113019775793\n",
      "artificial: 0.11785113019775793\n",
      "as: 0.11785113019775793\n",
      "be: 0.11785113019775793\n",
      "between: 0.11785113019775793\n",
      "can: 0.11785113019775793\n",
      "computational: 0.11785113019775793\n",
      "computers: 0.11785113019775793\n",
      "extraction: 0.11785113019775793\n",
      "field: 0.11785113019775793\n",
      "focuses: 0.11785113019775793\n",
      "for: 0.11785113019775793\n",
      "human: 0.11785113019775793\n",
      "humans: 0.11785113019775793\n",
      "information: 0.11785113019775793\n",
      "intelligence: 0.11785113019775793\n",
      "interaction: 0.11785113019775793\n",
      "involves: 0.11785113019775793\n",
      "is: 0.11785113019775793\n",
      "it: 0.11785113019775793\n",
      "language: 0.3535533905932738\n",
      "machine: 0.11785113019775793\n",
      "natural: 0.23570226039551587\n",
      "nlp: 0.23570226039551587\n",
      "of: 0.23570226039551587\n",
      "on: 0.11785113019775793\n",
      "processing: 0.11785113019775793\n",
      "question: 0.11785113019775793\n",
      "sentiment: 0.11785113019775793\n",
      "such: 0.11785113019775793\n",
      "tasks: 0.11785113019775793\n",
      "techniques: 0.11785113019775793\n",
      "that: 0.11785113019775793\n",
      "the: 0.23570226039551587\n",
      "to: 0.11785113019775793\n",
      "translation: 0.11785113019775793\n",
      "understand: 0.11785113019775793\n",
      "used: 0.11785113019775793\n",
      "using: 0.11785113019775793\n",
      "various: 0.11785113019775793\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([document])\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for (i,feature)in enumerate(feature_names):\n",
    "    tf_idf_value = tfidf_matrix[0,i]\n",
    "    if tf_idf_value>0:\n",
    "        print(f\"{feature}: {tf_idf_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef0dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
